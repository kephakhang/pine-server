from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,)
import pinecone
from langchain.embeddings.openai import OpenAIEmbeddings
from pinecone_text.sparse import BM25Encoder
import pandas as pd
import openai
from langchain.chains.llm import LLMChain
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT
from langchain.chains.question_answering import load_qa_chain
from langchain.chains import ConversationalRetrievalChain
from langchain.retrievers import PineconeHybridSearchRetriever
import os
from langchain.chat_models import ChatOpenAI
from item import Item
from notifier import Notifier
from streaming_callback import StreamingCallbackHandler
import logging
import re
import sqlite3
import ast


class PineconeSearch:

    def __init__(self, notifier: Notifier, logger):
        self.notifier: Notifier = notifier
        self.logger = logger
        # pinecone
        pinecone.init(
            api_key = 'xxx',
            environment = 'asia-southeast1-gcp'
        )
        self.index_name = "xxx"
        self.index_name_hotplc = "yyy"
        

        self.index = pinecone.Index(self.index_name)
        self.index_hotplc = pinecone.Index(self.index_name_hotplc)

        # openAI embedding
        self.model_name = 'text-embedding-ada-002'
        openai.api_key = 'xxx'
        self.embed = OpenAIEmbeddings(
            model=self.model_name,
            openai_api_key= 'yyy')

        # í•˜ë‚˜ì¹´ë“œ ì™¸êµ­ì¸ ë°ì´í„°
        self.df_hana = pd.read_excel('./ì™¸êµ­ì¸(ì¶”ì¶œ)_1023.xlsx')

        # BM25
        self.bm25 = BM25Encoder()

        # BM25 fit ë°ì´í„° 
        self.df = pd.read_excel('./FIT_0728.xlsx',dtype=str)

        self.bm25.fit(self.df['New_Column3'])

        os.environ["OPENAI_API_KEY"] = "xxx"
    
        self.retriever = PineconeHybridSearchRetriever(
            embeddings=self.embed, sparse_encoder=self.bm25, index=self.index, top_k= int(5), alpha= float(0.7))

        self.retriever_hotplc = PineconeHybridSearchRetriever(
            embeddings=self.embed, sparse_encoder=self.bm25, index=self.index_hotplc, top_k= int(5), alpha= float(0.7))

        self.general_system_template = self.read_prompt_db()

        print("prompt : ", self.general_system_template)

        # self.general_system_template = r""" 
        # Respond in Korean. Responding in English. You can only print up to 5 responses. All responses are printed in HTML format.You can only print up to 5 responses. All responses are printed in HTML format.In your response, summarize your store'\''s description and reviews in 150 characters or less to showcase each store'\''s features. Use emojis in your responses to illustrate your store'\''s features.\nIf you can'\''t include a store review in your response, then replace it with a short store description of 100 characters or less. If the store you'\''re looking for doesn'\''t exist in the source, clearly state that it doesn'\''t exist. if the question asks for '\''more'\'', find and print the next result, excluding the previous one. {context}

        # """
        self.llm = ChatOpenAI(model_name = "gpt-3.5-turbo-0613", temperature=0)
        self.streaming_llm = ChatOpenAI(model_name = "gpt-3.5-turbo-0613",streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)

        self.general_user_template = "Question:```{question}```"
        self.messages = [
                    SystemMessagePromptTemplate.from_template(self.general_system_template),
                    HumanMessagePromptTemplate.from_template(self.general_user_template)
        ]
        self.qa_prompt = ChatPromptTemplate.from_messages(self.messages )

        self.question_generator = LLMChain(llm=self.llm, prompt=CONDENSE_QUESTION_PROMPT)
        # doc_chain = load_qa_chain(llm, chain_type="stuff", prompt=qa_prompt)
        self.doc_chain = load_qa_chain(self.streaming_llm, chain_type="stuff", prompt=self.qa_prompt)

        self.qa = ConversationalRetrievalChain(
                    retriever=self.retriever, combine_docs_chain=self.doc_chain, question_generator=self.question_generator,
                    return_source_documents=True)
  
        self.qa_hotplc = ConversationalRetrievalChain(
                    retriever=self.retriever_hotplc, combine_docs_chain=self.doc_chain, question_generator=self.question_generator,
                    return_source_documents=True)

        self.chat_history = []
        self.logger.debug("Search instance is initialized")

    def read_prompt_db(self):
        con = sqlite3.connect("./pine.sqlite")
        cur = con.cursor()
        cur.execute("select prompt from tb_prompt limit 1")
        prompt = cur.fetchone()
        con.close()
        return prompt[0]
    
    def write_prompt_db(self, prompt:str):
        con = sqlite3.connect("./pine.sqlite")
        cur = con.cursor()
        cur.execute(f"update tb_prompt set prompt = ?, update_at = datetime('now','localtime') where id = 1",(prompt,))
        con.commit()
        con.close()

    def update_prompt(self, prompt:str):
        self.general_system_template = prompt
        self.write_prompt_db(prompt)
        # llm = ChatOpenAI(model_name = "gpt-3.5-turbo-0613", temperature=0)
        # streaming_llm = ChatOpenAI(model_name = "gpt-3.5-turbo-0613",streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)
        #
        # general_user_template = "Question:```{question}```"
        self.messages = [
                    SystemMessagePromptTemplate.from_template(self.general_system_template),
                    HumanMessagePromptTemplate.from_template(self.general_user_template)
        ]
        self.qa_prompt = ChatPromptTemplate.from_messages(self.messages )

        self.question_generator = LLMChain(llm=self.llm, prompt=CONDENSE_QUESTION_PROMPT)
        # doc_chain = load_qa_chain(llm, chain_type="stuff", prompt=qa_prompt)
        self.doc_chain = load_qa_chain(self.streaming_llm, chain_type="stuff", prompt=self.qa_prompt)

        self.qa = ConversationalRetrievalChain(
                    retriever=self.retriever, combine_docs_chain=self.doc_chain, question_generator=self.question_generator,
                    return_source_documents=True)
  
        self.qa_hotplc = ConversationalRetrievalChain(
                    retriever=self.retriever_hotplc, combine_docs_chain=self.doc_chain, question_generator=self.question_generator,
                    return_source_documents=True)

        self.chat_history = []
    
    def extract_keywords(self, query) :
            prompt = f"""
                1. Extract location & menu & food category keywords from above user query. show me just extracted keywords.
                2. you don't answer with user question. just Extract keywords
                3. List them separately!! only by space!!. Don't you double quotes (" ") and commas (,)
                4. For example : from. "ì—¬ì˜ë„ íšŒì‹ì¥ì†Œë¡œ ì¢‹ì€ ë§›ì§‘ ì¶”ì²œí•´ì¤˜"|to."ì—¬ì˜ë„ íšŒì‹ì¥ì†Œ", Don't show this exmaple.
                5. Answer in Korean only.""",
            query = "{}\n{}".format(query, prompt)
            response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages = [
                {'role': 'system', 
                'content': 'You are a highly intelligent system that extracts keywords from user questions.'},
                {"role": "user",
                "content": query}
                ],
            )
            output_keywords = response['choices'][0]['message']['content']
            extracted_keywords = re.sub(r'\b(?:ë§›ì§‘|ì¶”ì²œ)\b', '', output_keywords)
            extracted_keywords = extracted_keywords.strip().replace(',', '').strip()
            extracted_keywords = extracted_keywords.rstrip()
            
            return extracted_keywords
    def hana_forign(self,query) :

        stct_order_mapping = {'3ìŠ¤íƒ€': 0,'2ìŠ¤íƒ€': 1,'1ìŠ¤íƒ€': 2,'ë”í…Œì´ë¸”': 3}

        query_parts = query.split()
        query_parts = [part for part in query_parts if part != "ë­í‚¹"]
        ê´‘ì—­ì‹œë„ = None
        ì‹œêµ°êµ¬ = None
        ë²•ì •ë™ = None
        êµ­ì  = None

        # ê´‘ì—­ì‹œë„, ë²•ì •ë™, êµ­ì  ì¶”ì¶œ
        for part in query_parts:
            if part in self.df_hana['ê´‘ì—­ì‹œë„'].unique():
                ê´‘ì—­ì‹œë„ = part
            elif part in self.df_hana['ë²•ì •ë™'].unique() and part.endswith(('ê°€', 'ë™', 'ë¡œ', 'ë©´', 'ì')):
                ë²•ì •ë™ = part
            elif part in self.df_hana['êµ­ì '].unique():
                êµ­ì  = part

        # ì‹œêµ°êµ¬ ì¶”ì¶œ        
        remaining_parts = [part for part in query_parts if part not in [ê´‘ì—­ì‹œë„, ë²•ì •ë™, êµ­ì ]]
        remaining_parts = [' '.join(remaining_parts)]
        for part in remaining_parts:
            if part in self.df_hana['ì‹œêµ°êµ¬'].unique():
                ì‹œêµ°êµ¬ = part
                break


        try:
            ì¡°ê±´ = True

            if ê´‘ì—­ì‹œë„ is not None:
                ì¡°ê±´ &= (self.df_hana['ê´‘ì—­ì‹œë„'] == ê´‘ì—­ì‹œë„)
            if ì‹œêµ°êµ¬ is not None:
                ì¡°ê±´ &= (self.df_hana['ì‹œêµ°êµ¬'] == ì‹œêµ°êµ¬)
            if ë²•ì •ë™ is not None:
                ì¡°ê±´ &= (self.df_hana['ë²•ì •ë™'] == ë²•ì •ë™)
            if êµ­ì  is not None:
                ì¡°ê±´ &= (self.df_hana['êµ­ì '] == êµ­ì )

            df_result = self.df_hana[ì¡°ê±´].sort_values(by='ë§¤ì¶œê¸ˆì•¡', ascending=False).head(5)

            result_intro = ""
            if êµ­ì  is not None:
                if ë²•ì •ë™ is None:
                    result_intro = (f"2021ë…„ 9ì›”ë¶€í„° 2023ë…„ 7ì›”ê¹Œì§€ {ê´‘ì—­ì‹œë„} {ì‹œêµ°êµ¬} {êµ­ì }ì¸ì˜ ì†Œë¹„ ê¸ˆì•¡ ìˆœìœ„ì…ë‹ˆë‹¤.")
                    query_txt = (f"{ê´‘ì—­ì‹œë„} {ì‹œêµ°êµ¬}")
                else:
                    result_intro = (f"2021ë…„ 9ì›”ë¶€í„° 2023ë…„ 7ì›”ê¹Œì§€ {ê´‘ì—­ì‹œë„} {ì‹œêµ°êµ¬} {ë²•ì •ë™} {êµ­ì }ì¸ì˜ ì†Œë¹„ ê¸ˆì•¡ ìˆœìœ„ì…ë‹ˆë‹¤.")
                    query_txt = (f"{ê´‘ì—­ì‹œë„} {ì‹œêµ°êµ¬}")
            else:
                if ë²•ì •ë™ is None:
                    result_intro = (f"2021ë…„ 9ì›”ë¶€í„° 2023ë…„ 7ì›”ê¹Œì§€ {ê´‘ì—­ì‹œë„} {ì‹œêµ°êµ¬}ì˜ ì†Œë¹„ ê¸ˆì•¡ ìˆœìœ„ì…ë‹ˆë‹¤.")
                    query_txt = (f"{ê´‘ì—­ì‹œë„} {ì‹œêµ°êµ¬}")
                else:
                    result_intro = (f"2021ë…„ 9ì›”ë¶€í„° 2023ë…„ 7ì›”ê¹Œì§€ {ê´‘ì—­ì‹œë„} {ì‹œêµ°êµ¬} {ë²•ì •ë™}ì˜ ì†Œë¹„ ê¸ˆì•¡ ìˆœìœ„ì…ë‹ˆë‹¤.")
                    query_txt = (f"{ê´‘ì—­ì‹œë„} {ì‹œêµ°êµ¬}")

        except KeyError:
            result_intro = ("ì…ë ¥ëœ ì§€ì—­ ì •ë³´ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
        result_answer = ""
        if len(df_result) == 0:
            result_answer = "ì…ë ¥ëœ ì§€ì—­ ì •ë³´ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        else:
            result_intro = result_intro.replace('None', '')
            result_intro = result_intro.replace('  ', ' ')
            result_answer += result_intro + "\n\n"
            rank = 0
            for idx, row in df_result.iterrows():
                ìˆœìœ„ = f"{rank + 1}ìœ„"
                ìœ„ì¹˜ = f"ìœ„ì¹˜ : {row['ê´‘ì—­ì‹œë„']} {row['ì‹œêµ°êµ¬']} {row['ë²•ì •ë™']}"
                êµ­ì  = f"êµ­ì  : {row['êµ­ì ']}"
                ì—…ì¢… = f"{row['ì—…ì¢…ì†Œë¶„ë¥˜']}"
                ë§¤ì¶œê±´ìˆ˜ = int(row['ë§¤ì¶œê±´ìˆ˜']) 
                formatted_ë§¤ì¶œê±´ìˆ˜ = f"ë§¤ì¶œê±´ìˆ˜: {format(ë§¤ì¶œê±´ìˆ˜, ',')}"
                if rank == 0:
                    result_answer += f"{row['ê´‘ì—­ì‹œë„']} {row['ì‹œêµ°êµ¬']} {row['ë²•ì •ë™']}ì—ì„œ {row['êµ­ì ']}ì¸ì´ ê°€ì¥ ë§ì´ ì†Œë¹„í•œê³³ì€ {ì—…ì¢…}ì…ë‹ˆë‹¤.\n\n"
                result_answer += f"{ìˆœìœ„} : {ì—…ì¢…}\n{ìœ„ì¹˜}\n{êµ­ì }\n{formatted_ë§¤ì¶œê±´ìˆ˜}ê±´\n\n"
                rank += 1
            
            # ê²°ê³¼ë¬¼ ì˜ë¬¸ ë²ˆì—­ GPT
            prompt = f"""
                Please translate the foreign consumption rankings by region below into English. Please keep the ranking output result format the same.""",
            prompt_answer = "{}\n{}".format(prompt, result_answer)
            response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages = [
                {'role': 'system', 
                'content': 'You are an expert in translating ranking information.'},
                {"role": "user",
                "content": prompt_answer}
                ],
            )
            result_answer_2 = response['choices'][0]['message']['content']
        # ì™¸êµ­ì¸ ë°ì´í„° + ë§›ì§‘ê²€ìƒ‰
            query3 = query_txt


            streaming_llm = ChatOpenAI(model_name = "gpt-3.5-turbo-0613",streaming=True, temperature=0)
            # doc_chain = load_qa_chain(llm, chain_type="stuff", prompt=qa_prompt)
            doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=self.qa_prompt)

            qa = ConversationalRetrievalChain(
                retriever=self.retriever, combine_docs_chain=doc_chain, question_generator=self.question_generator,
                return_source_documents=True)

            qa_hotplc = ConversationalRetrievalChain(
                retriever=self.retriever_hotplc, combine_docs_chain=doc_chain, question_generator=self.question_generator,
                return_source_documents=True)



            result = qa({"question": query3, "chat_history": self.chat_history})
            result_hotplc = qa_hotplc({"question": query3, "chat_history": self.chat_history})

            result_with_dicts = {'source_documents': []}
            # í•˜ìœ„ í˜¸í™˜ì„±
            all_documents = result['source_documents'] + result_hotplc['source_documents']
            sorted_documents = sorted(all_documents, key=lambda doc: stct_order_mapping.get(doc.metadata.get('STCT', 'Unknown'), float('inf')))

            # ì‹ ê·œ ê²°ê³¼ : start-place ì™€ hot-place ë¡œ ë‚˜ëˆ”
            result['source_documents_star'] = sorted(result['source_documents'], key=lambda doc: stct_order_mapping.get(doc.metadata.get('STCT', 'Unknown'), float('inf')))
            result['source_documents_hotplc'] = sorted(result_hotplc['source_documents'], key=lambda doc: stct_order_mapping.get(doc.metadata.get('STCT', 'Unknown'), float('inf')))

            for document_obj in sorted_documents:
                page_content = document_obj.page_content
                metadata = document_obj.metadata
                new_dict = {'page_content': page_content, 'metadata': metadata}
                result_with_dicts['source_documents'].append(new_dict)

            


        final_result = {
            'question_ex': query,
            'chat_history': [],
            'answer_ex': result_answer_2,
            'source_documents': []
        }
        final_result['source_documents_ex'] = result_with_dicts['source_documents']
        
        return final_result

        
   
    def send_query(self, item: Item):
        # query2 = re.sub(r'(?<=[^\s])ë§›ì§‘|ë§›ì§‘(?=[^\s])', lambda m: ' ë§›ì§‘ ' if m.group(0) == 'ë§›ì§‘' else m.group(0).replace('ë§›ì§‘', ' ë§›ì§‘'), query)
        stct_order_mapping = {'3ìŠ¤íƒ€': 0,'2ìŠ¤íƒ€': 1,'1ìŠ¤íƒ€': 2,'ë”í…Œì´ë¸”': 3}

        try:
            self.logger.debug('qa start[')
            result: dict = {}
            
            default_mids = ['5487','5485','5484']
            predefined_queries = {
                "ì „ì°¸ì‹œ ì´ì˜ì ì»µëƒ‰ë©´ ğŸ‘€": {
                    "PID_list": ['847035', '131945', '361014', '631107', '67706'],
                    "Context": "ì´ë¯¸ ë„ˆ~~~ë¬´ ìœ ëª…í•œ 50ë…„ ì „í†µì˜ í‰ì–‘ëƒ‰ë©´ ëª…ê°€ ì„ë°€ëŒ€! ì „ì°¸ì‹œì— ë‚˜ì˜¨ ì—­ì‚¼ì  ì»µëƒ‰ë©´ë¶€í„° ê° ì§€ì—­ì— ë§›ì§‘ìœ¼ë¡œ ìë¦¬ì¡ì€ ì„ë°€ëŒ€ ì •ë³´ë¥¼ í•œ ëˆˆì— í™•ì¸í•´ë³´ì„¸ìš” ğŸ‘€"
                },
                "ì—¬ë¦„ì˜ ëìë½, ê¼­ ë¨¹ì–´ì•¼ í•  ë³´ì–‘ì‹": {
                    "PID_list": ['14803', '120768', '3033', '252596', '252789'],
                    "Context": "ì…ì¶”ë„ ì§€ë‚¬ëŠ”ë°... ì´ ë¬´ë”ìœ„ ì‹¤í™”?? ğŸ¥µğŸ¥µğŸ¥µ ì´ ë¬´ë”ìœ„ì—ì„œ ëª¸ì„ ê¸°ìš´ë‚˜ê²Œ í•´ ì¤„ ë³´ì–‘ì‹ ë§›ì§‘ì„ ì†Œê°œí•´ë“œë¦½ë‹ˆë‹¤."
                },
                "ì‹œì›í•œ ë§¥ì£¼ í•œ ì”ğŸº": {
                    "PID_list": ['481888', '351650', '373398', '354549', '1206881'],
                    "Context": "í‡´ê·¼í•˜ê³  ì‹œì›í•˜ê²Œ ë§¥ì£¼ í•œì”í•  ìƒê°ë§Œ í•´ë„ ì—…ë¬´ ëŠ¥ë¥ ì´ ğŸ”¥ğŸ”¥ğŸ”¥ ì´íƒœì›ì—ì„œ ê°€ì¥ ìœ ëª…í•œ ê°€ë§¥ì§‘ë¶€í„° ì„ì§€ë¡œ ê±°ë¦¬ë¥¼ ê°•ë‚¨ í•œë³µíŒì— ì¬í˜„í•œ ê°ì„± í˜¸í”„ì§‘ê¹Œì§€!"
                },
                "MZì„±ì§€ì…ì ì™„ë£Œ! SNS ë‹¬êµ° ë¹µì§‘": {
                    "PID_list": ['1286876', '1444441', '254092','1541987','1546259'],
                    "Context": "MZì„¸ëŒ€ì˜ ì„±ì§€ë¡œ ë¶ˆë¦¬ëŠ” ë¡¯ë°ì›”ë“œëª°ì— ë“œë””ì–´ ëŸ°ë˜ ë² ì´ê¸€ ë®¤ì§€ì—„ ì…ì  ì™„ë£ŒğŸ¥¯ğŸ‘¨â€ğŸ³ êµ­ë‚´ì— ë‹¨ 4ê³³ë§Œ ìˆëŠ” ì°ë§›ì§‘! ë‹¤ë¥¸ ê±´ ë‹¤ ì°¸ì•„ë„ ë¹µì€ ëª» ì°¸ì§€ ã„¹ã…‡ã…‹ã…‹ +++ ë¹µìˆœì´ë“¤ì„ ìœ„í•œ ì„œìš¸ 3ëŒ€ ë¹µì§‘ ì •ë³´ëŠ” ë¤!"
                },
                "ë†€ë©´ë­í•˜ë‹ˆ? ì „êµ­ê°„ì‹ìë‘ğŸŒ": {
                    "PID_list": ['540558', '402758', '254634','1477700','1176088'],
                    "Context": "ë†€ë©´ë­í•˜ë‹ˆì—ì„œ ì•¼ì‹¬ì°¨ê²Œ ì§„í–‰ì¤‘ì¸ ì „êµ­ê°„ì‹ì§€ë„ ì™„ì„±í•˜ê¸°! ì „ë¬¸ê°€(?)ë“¤ì´ ëª¨ì—¬ì„œ ë§Œë“  ì „êµ­ì— ìˆëŠ” ê°„ì‹ ë§›ì§‘ êµ¬ê²½í•˜ê¸°âœ¨"
                },
                "ì‹ì‹ AIì¶”ì²œì°ë§›ì§‘":{
                    "PID_list": ['316846','269870','148202','323','220819','1292068','1134007','1260730','1161671'],
                    "Context" : "ğŸ¤–ì‹ì‹  ë¹…ë°ì´í„°ì™€ ì¸ê³µì§€ëŠ¥ì´ ê²°í•©í•˜ì—¬ ì¶”ì²œë“œë¦¬ëŠ” ì°ë§›ì§‘ BEST5ì…ë‹ˆë‹¤. ë§›ì§‘ì„ ê³ ë¥´ê¸° í˜ë“œì‹œë‹¤ë©´ ì§€ê¸ˆ ì¶”ì²œí•´ë“œë¦¬ëŠ” ë§›ì§‘ì„ ê¼­ ì°¸ê³ í•˜ì„¸ìš”! ê±°ë¥¼ íƒ€ì„ ì´ í•˜ë‚˜ë„ ì—†ëŠ” â­ì°ë§›ì§‘â­ì…ë‹ˆë‹¤."
                },
                "ê°•ë‚¨ì§ì¥ì¸ì ì‹¬ë©”ë‰´":{
                    "PID_list": ['359781','360646','359368','336940','364849'],
                    "Context" : "ğŸ¤–ì‹ì‹  ë¹…ë°ì´í„°ì™€ ì¸ê³µì§€ëŠ¥ì´ ê²°í•©í•˜ì—¬ ì¶”ì²œë“œë¦¬ëŠ” ê°•ë‚¨ ì§ì¥ì¸ ì ì‹¬ë©”ë‰´ ì°ë§›ì§‘ BEST5ì…ë‹ˆë‹¤!."
                }
            }

            if item.question in predefined_queries:
                
                list_1 = predefined_queries[item.question]['PID_list']
                predefined_answer = predefined_queries[item.question]['Context']

                fetch_response = self.index.fetch(ids=list_1)
                fetch_response2 = self.index_hotplc.fetch(ids=list_1)
                fetch_response['vectors'].update(fetch_response2['vectors'])

                result['question'] = item.question
                result['chat_history'] = []
                result['answer'] = predefined_answer
                result['source_documents'] = []

                for list_item in list_1:
                    new_dic = {'page_content': fetch_response['vectors'][list_item]['metadata']['context'],
                            'metadata': fetch_response['vectors'][list_item]['metadata']}
                    result['source_documents'].append(new_dic)

                sorted_documents = sorted(result["source_documents"], key=lambda doc: stct_order_mapping.get(doc["metadata"]["STCT"], float('inf')))
                result['source_documents'] = sorted_documents

                # ì‹ ê·œ ê²°ê³¼ : star-place ì™€ hot-place ë¡œ ë‚˜ëˆ”
                result['source_documents_star'] = []
                result['source_documents_hotplc'] = sorted_documents

                mid_and_plc_list = [(doc['metadata']['MID'], doc['metadata']['PLC_ID']) for doc in result['source_documents']if 'MID' in doc['metadata']]
                result['magazine'] = []
                if len(mid_and_plc_list) < 3:
                    missing_count = 3 - len(mid_and_plc_list)
                    
                    for i in range(missing_count):
                        mid_and_plc_list.append((default_mids[i], ''))


                for mid, plc_id in mid_and_plc_list[:3]:
                    mid = [mid]
                    fetch_response = self.index.fetch(ids=mid, namespace='magazine')
                    metadata = fetch_response['vectors'][mid[0]]['metadata']
                    PID_dict = ast.literal_eval(metadata['PIDS'])
                    metadata['PIDS'] = PID_dict
                    
                    if plc_id in PID_dict:
                        plc_nm = PID_dict[plc_id]
                        metadata['PLC_NM'] = plc_nm

                    result['magazine'].append(metadata)


                return result
            elif "ë­í‚¹" in item.question :
                result = self.hana_forign(item.question)
                return result
            
            else:
                query2 = self.extract_keywords(item.question)

                if item.uid == None or item.uid == '':
                    qa = self.qa
                    qa_hotplc = self.qa_hotplc
                else:
                    if item.uid not in self.notifier.qa_dict:
                        streaming_llm = ChatOpenAI(model_name = "gpt-3.5-turbo-0613",streaming=True, callbacks=[StreamingCallbackHandler(self.notifier.connections[item.uid])], temperature=0)
                        # doc_chain = load_qa_chain(llm, chain_type="stuff", prompt=qa_prompt)
                        doc_chain = load_qa_chain(streaming_llm, chain_type="stuff", prompt=self.qa_prompt)

                        qa = ConversationalRetrievalChain(
                            retriever=self.retriever, combine_docs_chain=doc_chain, question_generator=self.question_generator,
                            return_source_documents=True)

                        qa_hotplc = ConversationalRetrievalChain(
                            retriever=self.retriever_hotplc, combine_docs_chain=doc_chain, question_generator=self.question_generator,
                            return_source_documents=True)

                        self.notifier.qa_dict[item.uid] = qa
                        self.notifier.qa_hotplc_dict[item.uid] = qa_hotplc
                    else:
                        qa = self.notifier.qa_dict[item.uid]
                        qa_hotplc = self.notifier.qa_hotplc_dict[item.uid]

                result = qa({"question": query2, "chat_history": self.chat_history})
                result_hotplc = qa_hotplc({"question": query2, "chat_history": self.chat_history})

                result_with_dicts = {'source_documents': []}
                # í•˜ìœ„ í˜¸í™˜ì„±
                all_documents = result['source_documents'] + result_hotplc['source_documents']
                sorted_documents = sorted(all_documents, key=lambda doc: stct_order_mapping.get(doc.metadata.get('STCT', 'Unknown'), float('inf')))

                # ì‹ ê·œ ê²°ê³¼ : start-place ì™€ hot-place ë¡œ ë‚˜ëˆ”
                result['source_documents_star'] = sorted(result['source_documents'], key=lambda doc: stct_order_mapping.get(doc.metadata.get('STCT', 'Unknown'), float('inf')))
                result['source_documents_hotplc'] = sorted(result_hotplc['source_documents'], key=lambda doc: stct_order_mapping.get(doc.metadata.get('STCT', 'Unknown'), float('inf')))

                for document_obj in sorted_documents:
                    page_content = document_obj.page_content
                    metadata = document_obj.metadata
                    new_dict = {'page_content': page_content, 'metadata': metadata}
                    result_with_dicts['source_documents'].append(new_dict)

                result['source_documents'] = result_with_dicts['source_documents']
                result['answer_hotplc'] = result_hotplc['answer']
                result['question'] = item.question

                mid_and_plc_list = [(doc['metadata']['MID'], doc['metadata']['PLC_ID']) for doc in result['source_documents']if 'MID' in doc['metadata']]
                
                result['magazine'] = []
                if len(mid_and_plc_list) < 3:
                    missing_count = 3 - len(mid_and_plc_list)
                    
                    for i in range(missing_count):
                        mid_and_plc_list.append((default_mids[i], ''))


                for mid, plc_id in mid_and_plc_list[:3]:
                    mid = [mid]
                    fetch_response = self.index.fetch(ids=mid, namespace='magazine')
                    metadata = fetch_response['vectors'][mid[0]]['metadata']
                    PID_dict = ast.literal_eval(metadata['PIDS'])
                    metadata['PIDS'] = PID_dict
                    
                    if plc_id in PID_dict:
                        plc_nm = PID_dict[plc_id]
                        metadata['PLC_NM'] = plc_nm

                    result['magazine'].append(metadata)

                self.logger.debug('qa end]')
                return result

        except Exception as e:
            return {
                "question": item.question,
                "answer": str(e),
                "source_documents": [],
                "chat_history": []
            }
